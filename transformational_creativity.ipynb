{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce4ebe9",
   "metadata": {},
   "source": [
    "# Generate Conceptual Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f4d22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp39-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in /Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages (from torch) (2025.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.7.1-cp39-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, sympy, MarkupSafe, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [torch]32m4/5\u001b[0m [torch]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d4bb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/samuelschapiro/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from concept_query_to_space import get_conceptual_space_from_concept\n",
    "from paper_to_space import get_conceptual_space_from_paper\n",
    "from entailment_score import compute_entailment\n",
    "from visualize_space import visualize_conceptual_space\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Initialize entailment model and tokenizer at import\n",
    "_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "\n",
    "def get_conceptual_space(input_str, input_type='concept', model=None, paper_name=None):\n",
    "    if input_type == 'concept':\n",
    "        concepts = get_conceptual_space_from_concept(input_str, model=model)\n",
    "    elif input_type == 'paper':\n",
    "        if paper_name is None:\n",
    "            concepts = get_conceptual_space_from_paper(\"None\", input_str, model=model)\n",
    "        else:\n",
    "            concepts = get_conceptual_space_from_paper(paper_name, input_str, model=model)\n",
    "    else:\n",
    "        raise ValueError(\"input_type must be either 'concept' or 'paper'\")\n",
    "    return {\n",
    "        'conceptual_space': concepts,\n",
    "        'compute_entailment': compute_entailment,\n",
    "        'entailment_tokenizer': _tokenizer,\n",
    "        'entailment_model': _model\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34407ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_space import visualize_conceptual_space\n",
    "\n",
    "paper_name=\"Attention is all you need\"\n",
    "\n",
    "concept = ''' The dominant sequence transduction models are based on complex recurrent or convolutional neura\n",
    "- The best performing models connect the encoder and decoder through an attention mechanism.\n",
    "- The Transformer is a new simple network architecture.\n",
    "- The architecture is based solely on attention mechanisms.\n",
    "- The architecture dispenses with recurrence and convolutions entirely.\n",
    "- These models are superior in quality.\n",
    "- These models are more parallelizable.\n",
    "- These models require significantly less time to train.\n",
    "- Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.\n",
    "- The model improves over the existing best results by over 2 BLEU.\n",
    "- Our model establishes a new single-model state-of-the-art BLEU score of 41.8.\n",
    "- The Transformer model generalizes well to other tasks.\n",
    "- Equal contribution is a fundamental principle.\n",
    "- Listing order is random.\n",
    "- Self-attention can replace RNNs.\n",
    "- Ashish and Illia contributed significantly to the development of the first Transformer models.\n",
    "- The design and implementation of tensor2tensor greatly improved results.\n",
    "- Tensor2tensor massively accelerated research.\n",
    "- Recurrent neural networks and their variants are state of the art in sequence modeling.\n",
    "- Long short-term memory and gated recurrent networks are particularly effective for language modelin\n",
    "- Recurrent models factor computation along symbol positions.\n",
    "- The inherently sequential nature of the process precludes parallelization within training examples.\n",
    "- Recent work has achieved significant improvements in computational efficiency.\n",
    "- Factorization tricks and conditional computation contribute to improvements in model performance.\n",
    "- The fundamental constraint of sequential computation remains.\n",
    "- Attention mechanisms are integral to sequence modeling and transduction models.\n",
    "- The Transformer is a model architecture that does not use recurrence.\n",
    "- The Transformer enables significantly more parallelization.\n",
    "- The Transformer achieves a new state of the art in translation quality.\n",
    "- Reducing sequential computation is a foundational goal in certain neural network architectures.\n",
    "- The number of operations required to relate signals from two arbitrary input or output positions varies\n",
    "- It is more difficult to learn dependencies between distant positions.\n",
    "- The Transformer reduces the number of operations to a constant number.\n",
    "- Self-attention (intra-attention) relates different positions of a single sequence.\n",
    "- Self-attention is effective in various tasks.\n",
    "- End-to-end memory networks utilize a recurrent attention mechanism.\n",
    "- End-to-end memory networks outperform sequence-aligned recurrence.\n",
    "- End-to-end memory networks are effective for simple-language question answering and language mo\n",
    "- The Transformer is the first transduction model relying entirely on self-attention.\n",
    "- The Transformer model is noteworthy.\n",
    "- Self-attention is a significant component of the Transformer.\n",
    "- The model functions in an auto-regressive manner.\n",
    "- The Transformer architecture utilizes stacked self-attention and fully connected layers.\n",
    "- The self-attention sub-layer in the decoder stack is modified.\n",
    "- The predictions for position i depend only on the known outputs at positions less than i.\n",
    "- An attention function can be described as mapping a query and a set of key-value pairs to an output.\n",
    "- core_claim\n",
    "- The specific attention mechanism being discussed is called 'Scaled Dot-Product Attention'.\n",
    "- Dot-product attention is identical to our algorithm.\n",
    "- Dot-product attention is faster and more space-efficient than other forms of attention.\n",
    "- Additive attention outperforms dot product attention without scaling for larger values of dk.\n",
    "- For large values of dk, the dot products grow large in magnitude.\n",
    "- Large dot products push the softmax function into regions with extremely small gradients.\n",
    "- It is beneficial to perform multiple attention functions instead of a single attention function.\n",
    "- The dot product of vectors q and k is represented by q·k=Pdk i=1qi ki.\n",
    "- The dot product has a mean of 0.\n",
    "- The dot product has a variance of dk.\n",
    "- Multi-head attention allows the model to jointly attend to information from different representation sub\n",
    "- Averaging inhibits the effectiveness of a single attention head.\n",
    "- The total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "- The Transformer model utilizes multi-head attention in multiple ways.\n",
    "- Every position in the decoder has the ability to attend over all positions in the input sequence.\n",
    "- The encoder contains self-attention layers.\n",
    "- In a self-attention layer, keys, values, and queries originate from the same source.\n",
    "- Each position in the encoder has the ability to attend to all positions in the previous layer.\n",
    "- Leftward information flow in the decoder must be prevented.\n",
    "- Preserving the auto-regressive property is essential.\n",
    "- Each layer in the encoder and decoder contains a feed-forward network.\n",
    "- Linear transformations are consistent across positions.\n",
    "- Learned embeddings are used to convert input and output tokens to vectors.\n",
    "- A model without recurrence and convolution requires additional information to understand the order o\n",
    "- Positional encodings have the same dimension as embeddings.\n",
    "- There are multiple types of positional encodings.\n",
    "- Each dimension of the positional encoding corresponds to a sinusoid.\n",
    "- The wavelengths form a geometric progression.\n",
    "- The chosen function enables the model to learn to attend by relative positions.\n",
    "- The sinusoidal version is preferable for model training.\n",
    "- core\n",
    "- The use of self-attention is motivated by certain desiderata.\n",
    "- The total computational complexity per layer is a significant concept.\n",
    "- The amount of computation that can be parallelized is significant.\n",
    "- The path length affects long-range dependencies in the network.\n",
    "- Learning long-range dependencies is a key challenge.\n",
    "- The length of the paths affects the ability to learn dependencies.\n",
    "- Shorter paths between positions in input and output sequences facilitate the learning of long-range d\n",
    "- There exists a maximum path length between any two input and output positions in networks compos\n",
    "- A self-attention layer connects all positions with a constant number of sequentially executed operatio\n",
    "- A recurrent layer requires O(n) sequential operations.\n",
    "- Self-attention layers are faster than recurrent layers in computational complexity when the sequence\n",
    "- Restricting self-attention can enhance computational performance.\n",
    "- Increasing the maximum path length affects computational performance.\n",
    "- A single convolutional layer with kernel width k < n does not connect all pairs of input and output pos\n",
    "- core_claim\n",
    "- Convolutional layers are generally more expensive than recurrent layers.\n",
    "- Separable convolutions decrease the complexity considerably.\n",
    "- The complexity of a separable convolution is equal to the combination of a self-attention layer and a p\n",
    "- Self-attention could yield more interpretable models.\n",
    "- Individual attention heads learn to perform different tasks.\n",
    "- Attention heads exhibit behavior related to the syntactic and semantic structure of sentences.\n",
    "- The Transformer achieves better BLEU scores than previous state-of-the-art models.\n",
    "- Increased uncertainty in a model can lead to improved accuracy.\n",
    "- The big transformer model outperforms the best previously reported models on the WMT 2014 Englis\n",
    "- A new state-of-the-art BLEU score of 28.4 has been established.\n",
    "- Our base model surpasses all previously published models and ensembles.\n",
    "- The big model achieves a BLEU score of 41.0.\n",
    "- The model outperforms all previously published single models.\n",
    "- The model does so at less than 1/4 of the training cost of the previous state-of-the-art model.\n",
    "- The number of floating point operations can be estimated based on training time, the number of GPU\n",
    "- The performance of the Transformer model can vary based on different components.\n",
    "- Unlisted values are identical to those of the base model.\n",
    "- Perplexities are defined on a per-wordpiece basis.\n",
    "- Single-head attention is less effective than the best setting by 0.9 BLEU.\n",
    "- Reducing the attention key size dkhurts model quality.\n",
    "- Determining compatibility is not easy.\n",
    "- A more sophisticated compatibility function than dot product may be beneficial.\n",
    "- Bigger models are better.\n",
    "- Dropout is helpful in avoiding over-fitting.\n",
    "- The Transformer can generalize to other tasks.\n",
    "- The task presents specific challenges.\n",
    "- The output is subject to strong structural constraints.\n",
    "- The output is significantly longer than the input.\n",
    "- RNN sequence-to-sequence models have limitations.\n",
    "- Our model performs surprisingly well despite the lack of task-specific tuning.\n",
    "- Our model yields better results than all previously reported models, except for the Recurrent Neural N\n",
    "- The Transformer outperforms the Berkeley-Parser.\n",
    "- The Transformer is the first sequence transduction model based entirely on attention.\n",
    "- The Transformer model is superior for translation tasks in terms of training speed.\n",
    "- Achieving a new state of the art in translation tasks.\n",
    "- Our best model outperforms all previously reported ensembles.\n",
    "- Attention-based models have a promising future.\n",
    "- The Transformer can be extended to other input and output modalities beyond text.\n",
    "- Making generation less sequential is a research goal.\n",
    "- Neural machine translation can be enhanced through joint learning of alignment and translation.\n",
    "- Recurrent neural networks can generate sequences.\n",
    "- Gradient flow in recurrent nets presents challenges.\n",
    "- Learning long-term dependencies is difficult.\n",
    "- Self-training PCFG grammars can be effectively utilized across languages.\n",
    "- Latent annotations enhance the capability of PCFG grammars.\n",
    "- There are limits to language modeling.\n",
    "- Active memory can potentially replace attention.\n",
    "- Neural GPUs learn algorithms.\n",
    "- Neural machine translation can be achieved in linear time.\n",
    "- A method for stochastic optimization\n",
    "- The notion of a structured self-attentive sentence embedding suggests a complexity inherent in langu\n",
    "- Attention-based neural machine translation is an effective approach.\n",
    "- Effective self-training is essential for parsing.\n",
    "- Attention models can be decomposed.\n",
    "- A deep reinforced model is effective for abstractive summarization.\n",
    "- Learning accurate, compact, and interpretable tree annotation is essential.\n",
    "- Using the output embedding can enhance the performance of language models.\n",
    "- Dropout is a method used to prevent overfitting in neural networks.\n",
    "- Grammar can be learned in the same way as a foreign language.'''\n",
    "\n",
    "result = get_conceptual_space(concept, input_type=\"paper\") # concept or paper\n",
    "concepts = result['conceptual_space']\n",
    "model = result['entailment_model']\n",
    "tokenizer = result['entailment_tokenizer']\n",
    "\n",
    "visualize_conceptual_space(concepts, show=True, entailment_model=model, entailment_tokenizer=tokenizer)  # show the graph\n",
    "# or\n",
    "# visualize_conceptual_space(concepts, show=False, save_path=\"conceptual_space.png\", entailment_model=model, entailment_tokenizer=tokenizer)  # save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870aaa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

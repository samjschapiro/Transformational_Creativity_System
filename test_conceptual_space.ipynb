{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9cd6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'conceptual_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconceptual_space\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_conceptual_space\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconceptual_space\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualize_space\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualize_conceptual_space\n\u001b[1;32m      5\u001b[0m paper_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention is all you need\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'conceptual_space'"
     ]
    }
   ],
   "source": [
    "import get_conceptual_space\n",
    "\n",
    "from conceptual_space.visualize_space import visualize_conceptual_space\n",
    "\n",
    "paper_name=\"Attention is all you need\"\n",
    "\n",
    "concept = ''' The dominant sequence transduction models are based on complex recurrent or convolutional neura\n",
    "- The best performing models connect the encoder and decoder through an attention mechanism.\n",
    "- The Transformer is a new simple network architecture.\n",
    "- The architecture is based solely on attention mechanisms.\n",
    "- The architecture dispenses with recurrence and convolutions entirely.\n",
    "- These models are superior in quality.\n",
    "- These models are more parallelizable.\n",
    "- These models require significantly less time to train.\n",
    "- Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.\n",
    "- The model improves over the existing best results by over 2 BLEU.\n",
    "- Our model establishes a new single-model state-of-the-art BLEU score of 41.8.\n",
    "- The Transformer model generalizes well to other tasks.\n",
    "- Equal contribution is a fundamental principle.\n",
    "- Listing order is random.\n",
    "- Self-attention can replace RNNs.\n",
    "- Ashish and Illia contributed significantly to the development of the first Transformer models.\n",
    "- The design and implementation of tensor2tensor greatly improved results.\n",
    "- Tensor2tensor massively accelerated research.\n",
    "- Recurrent neural networks and their variants are state of the art in sequence modeling.\n",
    "- Long short-term memory and gated recurrent networks are particularly effective for language modelin\n",
    "- Recurrent models factor computation along symbol positions.\n",
    "- The inherently sequential nature of the process precludes parallelization within training examples.\n",
    "- Recent work has achieved significant improvements in computational efficiency.\n",
    "- Factorization tricks and conditional computation contribute to improvements in model performance.\n",
    "- The fundamental constraint of sequential computation remains.\n",
    "- Attention mechanisms are integral to sequence modeling and transduction models.\n",
    "- The Transformer is a model architecture that does not use recurrence.\n",
    "- The Transformer enables significantly more parallelization.\n",
    "- The Transformer achieves a new state of the art in translation quality.\n",
    "- Reducing sequential computation is a foundational goal in certain neural network architectures.\n",
    "- The number of operations required to relate signals from two arbitrary input or output positions varies\n",
    "- It is more difficult to learn dependencies between distant positions.\n",
    "- The Transformer reduces the number of operations to a constant number.\n",
    "- Self-attention (intra-attention) relates different positions of a single sequence.\n",
    "- Self-attention is effective in various tasks.\n",
    "- End-to-end memory networks utilize a recurrent attention mechanism.\n",
    "- End-to-end memory networks outperform sequence-aligned recurrence.\n",
    "- End-to-end memory networks are effective for simple-language question answering and language mo\n",
    "- The Transformer is the first transduction model relying entirely on self-attention.\n",
    "- The Transformer model is noteworthy.\n",
    "- Self-attention is a significant component of the Transformer.\n",
    "- The model functions in an auto-regressive manner.\n",
    "- The Transformer architecture utilizes stacked self-attention and fully connected layers.\n",
    "- The self-attention sub-layer in the decoder stack is modified.\n",
    "- The predictions for position i depend only on the known outputs at positions less than i.\n",
    "- An attention function can be described as mapping a query and a set of key-value pairs to an output.\n",
    "- core_claim\n",
    "- The specific attention mechanism being discussed is called 'Scaled Dot-Product Attention'.\n",
    "- Dot-product attention is identical to our algorithm.\n",
    "- Dot-product attention is faster and more space-efficient than other forms of attention.\n",
    "- Additive attention outperforms dot product attention without scaling for larger values of dk.\n",
    "- For large values of dk, the dot products grow large in magnitude.\n",
    "- Large dot products push the softmax function into regions with extremely small gradients.\n",
    "- It is beneficial to perform multiple attention functions instead of a single attention function.\n",
    "- The dot product of vectors q and k is represented by qÂ·k=Pdk i=1qi ki.\n",
    "- The dot product has a mean of 0.\n",
    "- The dot product has a variance of dk.\n",
    "- Multi-head attention allows the model to jointly attend to information from different representation sub\n",
    "- Averaging inhibits the effectiveness of a single attention head.\n",
    "- The total computational cost is similar to that of single-head attention with full dimensionality.\n",
    "- The Transformer model utilizes multi-head attention in multiple ways.\n",
    "- Every position in the decoder has the ability to attend over all positions in the input sequence.\n",
    "- The encoder contains self-attention layers.\n",
    "- In a self-attention layer, keys, values, and queries originate from the same source.\n",
    "- Each position in the encoder has the ability to attend to all positions in the previous layer.\n",
    "- Leftward information flow in the decoder must be prevented.\n",
    "- Preserving the auto-regressive property is essential.\n",
    "- Each layer in the encoder and decoder contains a feed-forward network.\n",
    "- Linear transformations are consistent across positions.\n",
    "- Learned embeddings are used to convert input and output tokens to vectors.\n",
    "- A model without recurrence and convolution requires additional information to understand the order o\n",
    "- Positional encodings have the same dimension as embeddings.\n",
    "- There are multiple types of positional encodings.\n",
    "- Each dimension of the positional encoding corresponds to a sinusoid.\n",
    "- The wavelengths form a geometric progression.\n",
    "- The chosen function enables the model to learn to attend by relative positions.\n",
    "- The sinusoidal version is preferable for model training.\n",
    "- core\n",
    "- The use of self-attention is motivated by certain desiderata.\n",
    "- The total computational complexity per layer is a significant concept.\n",
    "- The amount of computation that can be parallelized is significant.\n",
    "- The path length affects long-range dependencies in the network.\n",
    "- Learning long-range dependencies is a key challenge.\n",
    "- The length of the paths affects the ability to learn dependencies.\n",
    "- Shorter paths between positions in input and output sequences facilitate the learning of long-range d\n",
    "- There exists a maximum path length between any two input and output positions in networks compos\n",
    "- A self-attention layer connects all positions with a constant number of sequentially executed operatio\n",
    "- A recurrent layer requires O(n) sequential operations.\n",
    "- Self-attention layers are faster than recurrent layers in computational complexity when the sequence\n",
    "- Restricting self-attention can enhance computational performance.\n",
    "- Increasing the maximum path length affects computational performance.\n",
    "- A single convolutional layer with kernel width k < n does not connect all pairs of input and output pos\n",
    "- core_claim\n",
    "- Convolutional layers are generally more expensive than recurrent layers.\n",
    "- Separable convolutions decrease the complexity considerably.\n",
    "- The complexity of a separable convolution is equal to the combination of a self-attention layer and a p\n",
    "- Self-attention could yield more interpretable models.\n",
    "- Individual attention heads learn to perform different tasks.\n",
    "- Attention heads exhibit behavior related to the syntactic and semantic structure of sentences.\n",
    "- The Transformer achieves better BLEU scores than previous state-of-the-art models.\n",
    "- Increased uncertainty in a model can lead to improved accuracy.\n",
    "- The big transformer model outperforms the best previously reported models on the WMT 2014 Englis\n",
    "- A new state-of-the-art BLEU score of 28.4 has been established.\n",
    "- Our base model surpasses all previously published models and ensembles.\n",
    "- The big model achieves a BLEU score of 41.0.\n",
    "- The model outperforms all previously published single models.\n",
    "- The model does so at less than 1/4 of the training cost of the previous state-of-the-art model.\n",
    "- The number of floating point operations can be estimated based on training time, the number of GPU\n",
    "- The performance of the Transformer model can vary based on different components.\n",
    "- Unlisted values are identical to those of the base model.\n",
    "- Perplexities are defined on a per-wordpiece basis.\n",
    "- Single-head attention is less effective than the best setting by 0.9 BLEU.\n",
    "- Reducing the attention key size dkhurts model quality.\n",
    "- Determining compatibility is not easy.\n",
    "- A more sophisticated compatibility function than dot product may be beneficial.\n",
    "- Bigger models are better.\n",
    "- Dropout is helpful in avoiding over-fitting.\n",
    "- The Transformer can generalize to other tasks.\n",
    "- The task presents specific challenges.\n",
    "- The output is subject to strong structural constraints.\n",
    "- The output is significantly longer than the input.\n",
    "- RNN sequence-to-sequence models have limitations.\n",
    "- Our model performs surprisingly well despite the lack of task-specific tuning.\n",
    "- Our model yields better results than all previously reported models, except for the Recurrent Neural N\n",
    "- The Transformer outperforms the Berkeley-Parser.\n",
    "- The Transformer is the first sequence transduction model based entirely on attention.\n",
    "- The Transformer model is superior for translation tasks in terms of training speed.\n",
    "- Achieving a new state of the art in translation tasks.\n",
    "- Our best model outperforms all previously reported ensembles.\n",
    "- Attention-based models have a promising future.\n",
    "- The Transformer can be extended to other input and output modalities beyond text.\n",
    "- Making generation less sequential is a research goal.\n",
    "- Neural machine translation can be enhanced through joint learning of alignment and translation.\n",
    "- Recurrent neural networks can generate sequences.\n",
    "- Gradient flow in recurrent nets presents challenges.\n",
    "- Learning long-term dependencies is difficult.\n",
    "- Self-training PCFG grammars can be effectively utilized across languages.\n",
    "- Latent annotations enhance the capability of PCFG grammars.\n",
    "- There are limits to language modeling.\n",
    "- Active memory can potentially replace attention.\n",
    "- Neural GPUs learn algorithms.\n",
    "- Neural machine translation can be achieved in linear time.\n",
    "- A method for stochastic optimization\n",
    "- The notion of a structured self-attentive sentence embedding suggests a complexity inherent in langu\n",
    "- Attention-based neural machine translation is an effective approach.\n",
    "- Effective self-training is essential for parsing.\n",
    "- Attention models can be decomposed.\n",
    "- A deep reinforced model is effective for abstractive summarization.\n",
    "- Learning accurate, compact, and interpretable tree annotation is essential.\n",
    "- Using the output embedding can enhance the performance of language models.\n",
    "- Dropout is a method used to prevent overfitting in neural networks.\n",
    "- Grammar can be learned in the same way as a foreign language.'''\n",
    "\n",
    "result = get_conceptual_space(concept, input_type=\"paper\") # concept or paper\n",
    "concepts = result['conceptual_space']\n",
    "model = result['entailment_model']\n",
    "tokenizer = result['entailment_tokenizer']\n",
    "\n",
    "visualize_conceptual_space(concepts, show=True, entailment_model=model, entailment_tokenizer=tokenizer)  # show the graph\n",
    "# or\n",
    "visualize_conceptual_space(concepts, show=False, save_path=\"conceptual_space.png\", entailment_model=model, entailment_tokenizer=tokenizer)  # save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f396191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
